{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_MznfcKK0GO",
        "outputId": "4d20a7fa-317f-4ddc-ed64-6949ad18871e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing add.cu\n"
          ]
        }
      ],
      "source": [
        " %%writefile add.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>  // Provides necessary functions and macros to work with CUDA.\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "__global__ void addVectors(int* A, int* B, int* C, int n) //__global__: Specifies that this function is a CUDA kernel, meaning it runs on the GPU and is called from the CPU.\n",
        "{\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x; // blockIdx.x: The index of the block within the grid. blockDim.x: The number of threads in each block. threadIdx.x: The index of the thread within the block.\n",
        "    if (i < n) // Ensures that threads do not access memory beyond the allocated array.\n",
        "    {\n",
        "        C[i] = A[i] + B[i]; // Adds corresponding elements from vectors A and B, storing the result in C.\n",
        "    }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    int n = 1000000; // The size of the vectors (one million elements).\n",
        "    int* A, * B, * C; // Pointers for the host (CPU) memory.\n",
        "    int size = n * sizeof(int); // The memory size required for each vector in bytes.\n",
        "\n",
        "    // Allocate memory on the host\n",
        "    cudaMallocHost(&A, size); //  Allocates pinned (page-locked) memory on the host. This improves the speed of memory transfer between host and device.\n",
        "    cudaMallocHost(&B, size);\n",
        "    cudaMallocHost(&C, size);\n",
        "\n",
        "    // Initialize the vectors\n",
        "    for (int i = 0; i < n; i++)\n",
        "    {\n",
        "        A[i] = i; // Fills A with values [0, 1, 2, ..., n-1].\n",
        "        B[i] = i * 2;  // Fills B with values [0, 2, 4, ..., 2*(n-1)].\n",
        "    }\n",
        "    // Allocate memory on the device\n",
        "    int* dev_A, * dev_B, * dev_C;\n",
        "    cudaMalloc(&dev_A, size); // cudaMalloc(&dev_A, size): Allocates size bytes of GPU memory for A.\n",
        "    cudaMalloc(&dev_B, size); // cudaMalloc(&dev_B, size): Allocates size bytes of GPU memory for B.\n",
        "    cudaMalloc(&dev_C, size); // cudaMalloc(&dev_C, size): Allocates size bytes of GPU memory for C.\n",
        "\n",
        "    // Copy data from host to device\n",
        "    cudaMemcpy(dev_A, A, size, cudaMemcpyHostToDevice); // cudaMemcpy(dest, src, size, cudaMemcpyHostToDevice): Copies data from host memory to device memory.\n",
        "    cudaMemcpy(dev_B, B, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch the kernel\n",
        "    int blockSize = 256; // Defines 256 threads per block.\n",
        "    int numBlocks = (n + blockSize - 1) / blockSize; // Ensures that all elements are covered\n",
        "    addVectors<<<numBlocks, blockSize>>>(dev_A, dev_B, dev_C, n); // This launches the kernel with numBlocks blocks and blockSize threads per block. Each thread computes C[i] = A[i] + B[i] in parallel.\n",
        "\n",
        "    // Copy data from device to host\n",
        "    cudaMemcpy(C, dev_C, size, cudaMemcpyDeviceToHost); // Copies the computed results from device memory (dev_C) to host memory (C).\n",
        "\n",
        "    // Print the results\n",
        "    for (int i = 0; i < 10; i++)\n",
        "    {\n",
        "        cout << C[i] << \" \"; // Prints the first 10 elements of C to verify the computation.\n",
        "    }\n",
        "    cout << endl;\n",
        "\n",
        "    // Free memory\n",
        "    cudaFree(dev_A); // releases memory on the GPU.\n",
        "    cudaFree(dev_B);\n",
        "    cudaFree(dev_C);\n",
        "    cudaFreeHost(A); //  releases pinned memory on the CPU.\n",
        "    cudaFreeHost(B);\n",
        "    cudaFreeHost(C);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /usr/local/cuda           # Removes any previous CUDA installations (only needed in certain environments).\n",
        "!ln -s  /usr/local/cuda-12.5 /usr/local/cuda        # Links to CUDA 12.2.\n",
        "!nvcc -arch=sm_75 add.cu -o add         # Compiles the CUDA program (nvcc is the CUDA compiler)."
      ],
      "metadata": {
        "id": "4pz10Ie6Lo0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./add // Each element of C is the sum of the corresponding elements of A and B:C[0] = 0 + 0 = 0    C[1] = 1 + 2 = 3   C[2] = 2 + 4 = 6    C[3] = 3 + 6 = 9    C[4] = 4 + 8 = 12    ..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXexO0J1MHov",
        "outputId": "1e2d852b-0123-4dca-e66e-440f4da0dfd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 3 6 9 12 15 18 21 24 27 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mjoWbDz8HSAM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}